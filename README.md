# Microsoft Malware Prediction

# Introduction:
Malware is a collective term for any kind of malicious software, designed to infiltrate and attack systems, servers or gain unauthorized access to networks. Some common types of malware are virus, worm, trojan horse, spyware and ransomware.

# Objective:
The goal is the predict the presence of a malware on a WIndows system

# Dataset:
Microsoft Corporation, Windows Defender ATP Research, Northeastern University College of
Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy
provides the dataset, through their competition hosted on Kaggle. The link to the competition is:
(https://www.kaggle.com/c/microsoft-malware-prediction/data).

# Methodology: 
This dataset has been put together by combining heartbeat and threat reports collected by Microsoft’s endpoint protection solutions, Windows Defender. The data contains properties of the machine and malware infections. There are 82 features in this dataset. The target variable is “HasDetections”. It indicates that if malware was detected on the machine. Each row in the dataset corresponds to a machine that can be uniquely identified by its “MachineIdentifier”.

Data wrangling: This involves cleaning and pre processing data for analysis, feature selection and building the dataset for modeling.

Modeling: Implementing machine learning models, such as, Random Forest Classifier, Logistic Regression, AdaBoost Classifier (with base estimator decision tree and logistic regression)

# Results:
In this project, I have implemented 4 machine learning models to help predict the presence of malware on Windows system. Random forest classifier and AdaBoost classifier with Decision Tree as base estimator have the same performance(65% accuracy) . Logistic regression and AdaBoost classifier with Logistic Regression as base estimator have same performance, but lower than that of random forest and AdaBoost(58% accuracy).

For future work, a combination of more feature engineering methods (multicollinearity, PCA) can be used to select the best features. Other boosting models such as XGBoost, Gradient boost classifiers can also be tested. Due to limitations in hardware, only a part of data was used for modeling. This can be addressed in future revisions.
